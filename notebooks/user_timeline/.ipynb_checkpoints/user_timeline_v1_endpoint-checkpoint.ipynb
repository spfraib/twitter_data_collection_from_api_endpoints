{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11556706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from glob import glob\n",
    "import json\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84efcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = os.path.join('/',os.getcwd().split('/')[1],'spf248','twitter_data_collection','data')\n",
    "start_pull = date.today().strftime(\"%d%m%Y\")\n",
    "id_type = 'user_id'\n",
    "max_tweets_window = 100000\n",
    "with open(os.path.join(path_to_data,'../../twitter_social_cohesion/data/data_collection','keys','proxies','proxy_key')) as f:\n",
    "    proxy_key = f.readlines()[0].strip('\\n|\"')\n",
    "    \n",
    "print('Date:',datetime.today().strftime('%d/%m/%Y %H:%M'))\n",
    "print('path_to_data:',path_to_data)\n",
    "print('start_pull:',start_pull)\n",
    "print('id_type:',id_type)\n",
    "print('max_tweets_window:',max_tweets_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxies(proxy_key):\n",
    "    \"\"\"\n",
    "    Get proxies from webshare.\n",
    "    Call the function at the top of script to get proxies.\n",
    "    Returns a list of formatted proxies.\n",
    "    \"\"\"\n",
    "\n",
    "    proxies_path = Path(\"proxies.txt\")\n",
    "    if not proxies_path.exists():\n",
    "        print(\"Retrieving proxies and saving as proxies.txt\")\n",
    "        # get proxies\n",
    "        resp = requests.get(\n",
    "            \"https://proxy.webshare.io/api/proxy/list/\",\n",
    "            headers={\"Authorization\": f\"Token {proxy_key}\"},\n",
    "        )\n",
    "        proxies = resp.json()[\"results\"]\n",
    "        for p in proxies:\n",
    "            prox = f\"http://{p['username']}:{p['password']}@{p['proxy_address']}:{p['ports']['http']}\"\n",
    "            p[\"proxy\"] = prox\n",
    "        proxies = pd.DataFrame(proxies)[\"proxy\"].to_list()\n",
    "        proxies_path.write_text(\"\\n\".join(proxies))\n",
    "    else:\n",
    "        print(\"Using cached proxy list proxies.txt\")\n",
    "        proxies = proxies_path.read_text().split(\"\\n\")\n",
    "\n",
    "    return proxies\n",
    "\n",
    "def change_proxy(proxylist, proxy_idx=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        proxylist (list): _description_\n",
    "        proxy_idx (int, optional): proxy index. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        int: proxy_idx\n",
    "    \"\"\"\n",
    "\n",
    "    # reset/remove proxies first\n",
    "    os.system(\"unset no_proxy\")\n",
    "    os.system(\"unset NO_PROXY\")\n",
    "    # os.system(\"unset OBJC_DISABLE_INITIALIZE_FORK_SAFETY\")\n",
    "\n",
    "    if proxy_idx is not None:\n",
    "        proxy = proxylist[proxy_idx]\n",
    "        print(f\"Switch to proxy {proxy_idx} ({proxy}))\")\n",
    "\n",
    "        os.environ[\"http_proxy\"] = proxy\n",
    "        os.environ[\"HTTP_PROXY\"] = proxy\n",
    "        os.environ[\"https_proxy\"] = proxy\n",
    "        os.environ[\"HTTPS_PROXY\"] = proxy\n",
    "\n",
    "    return proxy_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_key_files(path_to_data):\n",
    "    return sorted(glob(os.path.join(path_to_data,'../../twitter_social_cohesion/data/data_collection','keys','v1','oauth1','apps','*.json')))\n",
    "    \n",
    "key_files = select_key_files(path_to_data)\n",
    "print('key files:', len(key_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f583742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_var(varname,default):\n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname,':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname,':', var,'(Default)')\n",
    "    return var\n",
    "\n",
    "# Choose Number of Nodes To Distribute Credentials: e.g. jobarray=0-4, cpu_per_task=20, credentials = 90 (<100)\n",
    "SLURM_JOB_ID            = get_env_var('SLURM_JOB_ID',0)\n",
    "SLURM_ARRAY_TASK_ID     = get_env_var('SLURM_ARRAY_TASK_ID',0)\n",
    "SLURM_ARRAY_TASK_COUNT  = get_env_var('SLURM_ARRAY_TASK_COUNT',len(key_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53771441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this before making any twitter request\n",
    "proxies = get_proxies(proxy_key)\n",
    "# change_proxy(proxies, proxy_idx=None)  # doesn't do anything\n",
    "change_proxy(proxies, proxy_idx=SLURM_ARRAY_TASK_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_file(key_files,SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT):\n",
    "    print('# keys:', len(key_files))\n",
    "    if SLURM_ARRAY_TASK_COUNT!=len(key_files) or SLURM_ARRAY_TASK_ID>=len(key_files) or SLURM_ARRAY_TASK_ID<0:\n",
    "        print(\"CHECK JOBARRAY\")\n",
    "    return key_files[SLURM_ARRAY_TASK_ID]\n",
    "        \n",
    "key_file = get_key_file(key_files,SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT)\n",
    "print('key file:', key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_API_auth(key_file):\n",
    "    # Import Key\n",
    "    with open(key_file) as f:\n",
    "        key = json.load(f)\n",
    "    # OAuth process, using the keys and tokens\n",
    "    auth = tweepy.OAuthHandler(key['API_Key'], key['API_Key_Secret'])\n",
    "    auth.set_access_token(key['Access_Token'], key['Access_Token_Secret'])\n",
    "    # Creation of the actual interface, using authentication\n",
    "    api_auth = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    try:\n",
    "        api_auth.verify_credentials()\n",
    "    except:\n",
    "        print(key_file,\": error during authentication\")\n",
    "    return api_auth\n",
    "\n",
    "# for key_file in key_files:\n",
    "#     get_API_auth(key_file)\n",
    "# print('Credentials Checked!')\n",
    "\n",
    "# Create API auth\n",
    "api = get_API_auth(key_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ea311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_requests_for_full_timeline(user_timeline,count=200):\n",
    "    if user_timeline.shape[0]:\n",
    "        return user_timeline.shape[0]//count+(user_timeline.shape[0]%count>0)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def pull_from_user_timeline_API_endpoint(api,user,id_type,since_id):\n",
    "    user_timeline=[]\n",
    "    error=None\n",
    "    try:\n",
    "        if since_id:\n",
    "            if id_type == 'user_id':\n",
    "                cursor=tweepy.Cursor(api.user_timeline,user_id=user,since_id=since_id,count=200,tweet_mode=\"extended\",include_rts=True).items()\n",
    "            elif id_type == 'screen_name':\n",
    "                cursor=tweepy.Cursor(api.user_timeline,screen_name=user,since_id=since_id,count=200,tweet_mode=\"extended\",include_rts=True).items()\n",
    "        else:\n",
    "            if id_type == 'user_id':\n",
    "                cursor=tweepy.Cursor(api.user_timeline,user_id=user,count=200,tweet_mode=\"extended\",include_rts=True).items()\n",
    "            elif id_type == 'screen_name':\n",
    "                cursor=tweepy.Cursor(api.user_timeline,screen_name=user,count=200,tweet_mode=\"extended\",include_rts=True).items()\n",
    "        for status in cursor:\n",
    "            user_timeline.append(status._json)\n",
    "    except tweepy.errors.TweepyException as e:\n",
    "        error = str(e)\n",
    "    return pd.DataFrame(user_timeline), error\n",
    "\n",
    "# timeline_test_1,error = pull_from_user_timeline_API_endpoint(api,'@deaneckles','screen_name','1388683708195086336')\n",
    "# timeline_test_2,error = pull_from_user_timeline_API_endpoint(api,'@deaneckles','screen_name',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719225e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pull_users(users,country,pull_type):\n",
    "    n_users_queried = 0\n",
    "    n_users_pulled = 0\n",
    "    n_users_error = 0\n",
    "    output_id = str(uuid.uuid4())\n",
    "    tweets_window = pd.DataFrame()\n",
    "    n_tweets_window = 0\n",
    "    n_requests_window = 0\n",
    "    n_users_window = 0\n",
    "    start = timer()\n",
    "    for user_id,tweet_id in users.to_records(index=False):\n",
    "        print('# queried users:',n_users_queried)\n",
    "        print('# pulled users:',n_users_pulled)\n",
    "        print('# pulled missed:',n_users_error)\n",
    "        print('# pulled users in window:',n_users_window)\n",
    "        print('# pulled requests in window:',n_requests_window)\n",
    "        print('# pulled tweets in window:',n_tweets_window)\n",
    "        n_users_queried += 1\n",
    "        tweets_user, error = pull_from_user_timeline_API_endpoint(api,user_id,id_type,tweet_id)\n",
    "        if not error:\n",
    "            tweets_window = pd.concat([tweets_window, tweets_user],sort=False)\n",
    "            n_tweets_window += tweets_user.shape[0]\n",
    "            n_requests_window += n_requests_for_full_timeline(tweets_user)\n",
    "            n_users_window += 1\n",
    "            n_users_pulled += 1\n",
    "        else:\n",
    "            n_users_error += 1\n",
    "            print('Error pulling user',n_users_queried,':',error)\n",
    "        if n_tweets_window > max_tweets_window or user_id == users.to_records(index=False)[-1][0]:\n",
    "            dirname = os.path.join(path_to_data,'user_timeline','API',country,start_pull,pull_type)\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "            filename = 'user_timeline_'+str(n_users_window)+'_users_'+str(n_requests_window)+'_requests_'+str(n_tweets_window)+'_tweets_'+start_pull+'_'+pull_type+'_'+str(SLURM_JOB_ID)+'_'+str(SLURM_ARRAY_TASK_ID)+'_'+output_id+'.json.bz2'\n",
    "            tweets_window.to_json(\n",
    "            os.path.join(dirname,filename),\n",
    "            orient='records',\n",
    "            force_ascii=False,\n",
    "            date_format=None,\n",
    "            double_precision=15)\n",
    "            end = timer()\n",
    "            print('Saved in', filename)\n",
    "            print('Computing Time:', round(end - start), 'sec')\n",
    "            print()\n",
    "            output_id = str(uuid.uuid4())\n",
    "            tweets_window = pd.DataFrame()\n",
    "            n_tweets_window = 0\n",
    "            n_requests_window = 0\n",
    "            n_users_window = 0\n",
    "            start = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f994a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Load and select users:')\n",
    "start = timer()\n",
    "\n",
    "files = pd.Series(sorted(glob(os.path.join(path_to_data,'user_timeline','batch','*parquet')))).sample(frac=1,random_state=0).to_list()\n",
    "tmp = pd.read_parquet(list(np.array_split(files,SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID]))\n",
    "\n",
    "for pull_type in ['update','full']:\n",
    "    print('Pull type:',pull_type)\n",
    "    for country in ['MX','PK']:\n",
    "        print('Country:',country)\n",
    "        users = tmp[tmp['country_short']==country][['user_id','tweet_id']]\n",
    "        if pull_type=='update':\n",
    "            users.dropna(inplace=True)\n",
    "        elif pull_type=='full':\n",
    "            users.drop(users.dropna().index,inplace=True)\n",
    "        users = users.sort_values(by='user_id').sample(frac=1,random_state=0).reset_index(drop=True)\n",
    "        print('# users:', len(users))\n",
    "        pull_users(users,country,pull_type)\n",
    "        \n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7efae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
